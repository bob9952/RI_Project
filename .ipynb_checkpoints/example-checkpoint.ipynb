{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a8a183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9864dd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "symbols_list = ['zero','one','two','three','four','five','six','seven','eight','nine','minus','plus','equal','div','decimal','times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a171c02f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128,128) # 155x135 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a6ed1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2912a7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = \"train/\"\n",
    "# train_image = []\n",
    "# train_label = []\n",
    "\n",
    "# for symbols_dir in os.listdir(dataset_path):\n",
    "#     if symbols_dir.split()[0] in symbols_list:\n",
    "#         for image in os.listdir(os.path.join( dataset_path, symbols_dir)):\n",
    "#             train_label.append(symbols_dir.split()[0])\n",
    "#             train_image.append(os.path.join( dataset_path, symbols_dir, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ebeca8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_path = \"eval/\"\n",
    "# test_image = []\n",
    "# test_label = []\n",
    "\n",
    "# for symbols_dir in os.listdir(eval_path):\n",
    "#     if symbols_dir.split()[0] in symbols_list:\n",
    "#         for image in os.listdir( os.path.join (eval_path, symbols_dir)):\n",
    "#             test_label.append(symbols_dir.split()[0])\n",
    "#             test_image.append(os.path.join( eval_path, symbols_dir, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c792c4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train = []\n",
    "# X_test = []\n",
    "\n",
    "# # laoding the images from the path\n",
    "# for path in tqdm(train_image):    \n",
    "#     img = cv2.imread(path)\n",
    "#     img = cv2.resize(img, IMAGE_SIZE)\n",
    "#     img = np.array(img)\n",
    "#     X_train.append(img)\n",
    "\n",
    "# for path in tqdm(test_image):    \n",
    "#     img = cv2.imread(path)\n",
    "#     img = cv2.resize(img, IMAGE_SIZE)\n",
    "#     img = np.array(img)     \n",
    "#     X_test.append(img)\n",
    "\n",
    "# # creating numpy array from the images\n",
    "# X_train = np.array(X_train)\n",
    "# X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc98586b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # normalizing the data\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "\n",
    "# X_train = 1 - X_train\n",
    "# X_test = 1 - X_test\n",
    "# print(\"X_train shape: \", X_train.shape)\n",
    "# print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff28ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(train_image[0])\n",
    "# print(train_label[0])\n",
    "# plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fa03c-6b9c-4600-bc7a-5735a34cd9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc9fbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_train = np.array([[0] * 16 for _ in range(len(train_label))])\n",
    "\n",
    "# for i, category in enumerate(train_label):\n",
    "#     index = symbols_list.index(category)\n",
    "#     y_train[i][index] = 1\n",
    "#     y_test = np.array([[0] * 16 for _ in range(len(test_label))])\n",
    "\n",
    "# for i, category in enumerate(test_label):\n",
    "#     index = symbols_list.index(category)\n",
    "#     y_train[i][index] = 1\n",
    "\n",
    "# print(y_test.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762675e1-f94c-4aa3-a497-bc69981acf1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04cd4aac-73c0-42c5-a248-8635c00a431a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7557 images belonging to 16 classes.\n",
      "Found 1010 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "Train_Datagen = ImageDataGenerator(dtype = 'float32', preprocessing_function=keras.applications.resnet.preprocess_input)\n",
    "Val_Datagen = ImageDataGenerator(dtype = 'float32', preprocessing_function=keras.applications.resnet.preprocess_input)\n",
    "\n",
    "train_gen = Train_Datagen.flow_from_directory(directory = './train', target_size = (224, 224), color_mode = 'rgb', \n",
    "                                       batch_size = batch_size, class_mode = 'categorical', shuffle = True, seed = 42)\n",
    "\n",
    "val_gen = Val_Datagen.flow_from_directory(directory = './eval', target_size = (224, 224), color_mode = 'rgb',\n",
    "                                       batch_size = batch_size, class_mode = 'categorical', shuffle = True, seed = 42)\n",
    "\n",
    "epochs = 15\n",
    "Number_Of_Training_Images = train_gen.classes.shape[0]\n",
    "steps_per_epoch = Number_Of_Training_Images/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207132d-120e-4405-b35a-7d891dd6a3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3763ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###  MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8bcbaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4863feb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfer_model = ResNet50(include_top=False, pooling='avg', weights='imagenet', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14275459-cd39-404a-a12f-f30add2136a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transfer_model.load_weights(filepath=resnet_weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49030631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transfer_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe46ae30-8970-48fa-befb-c2622dd0de76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for layer in transfer_model.layers[:-8]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d7e0e-f1c6-4281-881c-cede811cfbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8735e14-a716-437d-9495-38594deac715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d22021b-968a-4fd0-8373-71412adcbd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                4112      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,116,368\n",
      "Trainable params: 3,944,208\n",
      "Non-trainable params: 20,172,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(transfer_model)\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units = 256, activation = 'relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units = 16, activation = 'softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b76ee6-1d5f-4fee-bc04-2d744a9d6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size\n",
    "STEP_SIZE_VALID=val_gen.n//val_gen.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9338b-3907-4e26-a10f-bf48b16d340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'Adam', loss = keras.metrics.categorical_crossentropy, metrics = ['accuracy'])\n",
    "\n",
    "history2 = model2.fit_generator(generator=train_gen, epochs = epochs, \n",
    "                    #batch_size = batch_size,\n",
    "                    validation_data = val_gen, steps_per_epoch = STEP_SIZE_TRAIN, validation_steps = STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9968817-6600-4596-9971-596ba2597738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#x1 = keras.layers.MaxPooling1D()(transfer_model.output)\n",
    "# transfer_model.layers[-1].trainable = True\n",
    "# transfer_model.layers[-2].trainable = True\n",
    "#x1 = keras.layers.Flatten()(transfer_model.output)\n",
    "#x1 = keras.layers.Dropout(0.5)(x1)\n",
    "#x1 = keras.layers.Dense(64, activation='relu')(x1)\n",
    "#x1 = keras.layers.Dropout(0.5)(x1)\n",
    "#x1 = keras.layers.Dense(16, activation='softmax')(x1)\n",
    "\n",
    "#model1 = keras.models.Model(inputs=transfer_model.input, outputs=x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4caf89-c9bb-4b77-8a96-ad0cdd3215e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3acb6-525a-458c-9d8c-434452be5717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model1.compile(optimizer=keras.optimizers.Adam(), loss=keras.metrics.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7ded9-9985-40ca-bb0f-5b7d31b39f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801ff63-a575-48d4-b4c9-87cb871cc753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#history2 = model1.fit(\n",
    "#    X_train,\n",
    "#    y_train,\n",
    "#    epochs=10,\n",
    "#    batch_size=16,\n",
    "#    validation_split=0.2\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6571be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#inputs = keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1],3))\n",
    "#x = transfer_model(inputs)\n",
    "#x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "#x = keras.layers.Flatten()(x)\n",
    "#outputs = keras.layers.Dense(16, activation='softmax')(x)\n",
    "#model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea31bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfbd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=keras.metrics.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c0271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#history = model.fit(\n",
    "#    X_train,\n",
    "#    y_train,\n",
    "#    epochs=20,\n",
    "#    batch_size=16\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c360614-1b61-4289-81ee-472c0d0555e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e137a58-f5d3-4df6-9191-3d6b0bc27013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#img_path = '../projekat/characters/8.png'\n",
    "#img = image.load_img(img_path, target_size=(32, 32))\n",
    "#x = image.img_to_array(img)\n",
    "#x = np.expand_dims(x, axis=0)\n",
    "#preds = model.predict(x)\n",
    "\n",
    "#print(symbols_list[preds.argmax(axis=1)[0]])\n",
    "#print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f1d9a-9d81-4bde-bad8-38c908c64076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI Project 2",
   "language": "python",
   "name": "ri_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
